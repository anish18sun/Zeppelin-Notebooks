{
  "paragraphs": [
    {
      "text": "%md\n# **WorldWideWeb**\n\nby Anish Singh\nblog site : [Zeppelin Notes](http://zeppelinnotes.blogspot.in)\n[Image]\n\n#### Introduction\n**WorldWideWeb** is a notebook that explores web crawl data and performs analysis on that data to gain interesting insights. The data used for the analysis is open data made available by the\ncommon crawl organization which provides web crawl data for free and open use. The common crawl organization builds and maintains an open repository web crawl data that can be  accessed  and\nanalysed by anyone. They have the web crawl data for the last seven years and the data is available in three formats : the WARC format, containing the raw crawl data, the WAT format, contain\n-ing the metadata for the crawled data and the WET format, containing the text extracted from the crawled data. The WARC format has largely been used for analysis in this  notebook  and  the\nother formats have been used as and when required. For the analysis and manipulation of the Warc format files, no tool other than the Warcbase library could have been more useful.  The  data\nfor only the month of May 2016 has been considered as the entire corpus is too large to be analysed at once, considering the fact that this notebook was running on a m4.xlarge EC2  instance.\nThe data for the month of May is divided into segments and only six segments have been considered, again due to the restricting capabilities of the machine used to design this notebook. More\ninformation about the design of the notebook and choosing of the data sets may be found on the blogsite mentioned above.\n\nThis notebook uses Apache Spark for analysis of common crawl data and the visualization capabilities of Apache Zeppelin.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 7:29:12 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468654360603_553258185",
      "id": "20160716-073240_1314434417",
      "dateCreated": "Jul 16, 2016 7:32:40 AM",
      "dateStarted": "Jul 27, 2016 7:29:12 AM",
      "dateFinished": "Jul 27, 2016 7:29:12 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n#this paragraph downlaods the common crawl datasets needed for analysis\n\n#download the datasets for the first three segments of May\n wget -O ~/ZeppelinData/CommonCrawl/MayWarcSeg1.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464049270134.8/warc/CC-MAIN-20160524002110-00000-ip-10-185-217-139.ec2.internal.warc.gz\"\n wget -O ~/ZeppelinData/CommonCrawl/MayWarcSeg2.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464049270134.8/warc/CC-MAIN-20160524002110-00001-ip-10-185-217-139.ec2.internal.warc.gz\"\n wget -O ~/ZeppelinData/CommonCrawl/MayWarcSeg3.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464049270134.8/warc/CC-MAIN-20160524002110-00000-ip-10-185-217-139.ec2.internal.warc.gz\"\n \n#download the datasets for the last two segments of May\n wget -O ~/ZeppelinData/commonCrawl/MayWarcSeg4.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464056639771.99/warc/CC-MAIN-20160524022359-00242-ip-10-185-217-139.ec2.internal.warc.gz\"\n wget -O ~/ZeppelinData/commonCrawl/MayWarcSeg5.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464056639771.99/warc/CC-MAIN-20160524022359-00243-ip-10-185-217-139.ec2.internal.warc.gz\"\n wget -O ~/ZeppelinData/commonCrawl/MayWarcSeg6.warc.gz \"https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2016-22/segments/1464056639771.99/warc/CC-MAIN-20160524022359-00244-ip-10-185-217-139.ec2.internal.warc.gz\"\n ",
      "dateUpdated": "Jul 16, 2016 12:51:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468654366832_874154092",
      "id": "20160716-073246_1723222826",
      "dateCreated": "Jul 16, 2016 7:32:46 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\n//this paragraph loads the warcbase library for analysis of warc files \nz.load(\"/home/ubuntu/Softwares/warcbase/warcbase-core/target/warcbase-core-0.1.0-SNAPSHOT-fatjar.jar\")          //load the warcbase library \nz.load(\"/home/ubuntu/Softwares/machinelearning/stanford-ner-2015-12-09/stanford-ner.jar\")                       //load the stanford nlp library\n\n//load the lucene libraries\nz.load(\"/home/ubuntu/Softwares/lucene/lucene-6.1.0/core/lucene-core-6.1.0.jar\")\nz.load(\"/home/ubuntu/Softwares/lucene/lucene-6.1.0/queryparser/lucene-queryparser-6.1.0.jar\")\nz.load(\"/home/ubuntu/Softwares/lucene/lucene-6.1.0/analysis/common/lucene-analyzers-common-6.1.0.jar\")\n",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 3:28:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468675569903_1868297183",
      "id": "20160716-132609_624363099",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@70ce5fc9\n"
      },
      "dateCreated": "Jul 16, 2016 1:26:09 PM",
      "dateStarted": "Aug 7, 2016 3:28:26 AM",
      "dateFinished": "Aug 7, 2016 3:28:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph loads the datasets in zeppelin for analysis\nimport org.warcbase.spark.matchbox._\nimport org.warcbase.spark.rdd.RecordRDD._\n\nval mayWarcSeg1 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg1.warc.gz\", sc)        //load the data sets for the first three segments of May\nval mayWarcSeg2 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg2.warc.gz\", sc)\nval mayWarcSeg3 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg3.warc.gz\", sc)\n\nval mayBegData \u003d mayWarcSeg1.union(mayWarcSeg2).union(mayWarcSeg3)                     //combine the data for the first three segments of May\n\nval mayWarcSeg4 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg4.warc.gz\", sc)        //load the data sets for the last three segments of May\nval mayWarcSeg5 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg5.warc.gz\", sc)\nval mayWarcSeg6 \u003d RecordLoader.loadArchives(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWarcSeg6.warc.gz\", sc)\n\nval mayEndData \u003d mayWarcSeg4.union(mayWarcSeg5).union(mayWarcSeg6)                     //combine the data for the last three segments of May\n\nval mayData \u003d mayBegData.union(mayEndData)                                             //combine data for the beginning and the ending segments of May",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 3:28:38 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468675947869_509083299",
      "id": "20160716-133227_1365870344",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.warcbase.spark.matchbox._\nimport org.warcbase.spark.rdd.RecordRDD._\nmayWarcSeg1: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[2] at map at RecordLoader.scala:45\nmayWarcSeg2: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[5] at map at RecordLoader.scala:45\nmayWarcSeg3: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[8] at map at RecordLoader.scala:45\nmayBegData: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d UnionRDD[10] at union at \u003cconsole\u003e:41\nmayWarcSeg4: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[13] at map at RecordLoader.scala:45\nmayWarcSeg5: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[16] at map at RecordLoader.scala:45\nmayWarcSeg6: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[19] at map at RecordLoader.scala:45\nmayEndData: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d UnionRDD[21] at union at \u003cconsole\u003e:41\nmayData: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d UnionRDD[22] at union at \u003cconsole\u003e:51\n"
      },
      "dateCreated": "Jul 16, 2016 1:32:27 PM",
      "dateStarted": "Aug 7, 2016 3:28:38 AM",
      "dateFinished": "Aug 7, 2016 3:28:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 1. Domain Frequency\nThe domain frequencies as extracted from the web crawl data would be a good indicator of the presense of various domains on the web and the extent to which they are spread. This introductory\nsection attempts to examine the domain frequencies to get an idea about the domains that have been more prominent in the crawl data under consideration for both the introductory and the endi\n-ng sections of May. The paragraph below first generates the data needed for the visualization (generates the frequency data) and the paragraphs that follow display the data. For the beginni\n-ng period of May, we find that the domains : \u0027fangraphs.com\u0027, \u0027osnews.com\u0027, \u0027packages.ubuntu.com\u0027 and \u0027google.com\u0027 are on the top. \u0027fangraphs.com\u0027 is a site for baseball news and blog while\n\u0027osnews.com\u0027 is based on blogs and reports about products and companies in the computer industry. The remaining top two domains hold their positions for well understood reasons.\n\nAs for the ending sections of May, the domains change their relative positions with \u0027economist.com\u0027 taking the top position, followed by \u0027build.com\u0027, \u0027dailytech.com\u0027 and \u0027theday.com\u0027.  These\nwebsites too are news and blog sites with the \u0027economist.com\u0027 concerned with the economics of the world; \u0027dailytech.com\u0027 is again a news and blog site for the information about the  computer\nindustry; only \u0027build.com\u0027 is a site among the others that is not a news website and offers solutions for building homes and decor of houses. The websites that were at the top in the beginni\n-ng have receeded to lower positions for the ending segments data.\n\n\u003e *More Frequency may get more Ad revenues*\n\nAdvertisement is undoubtfully one of the greatest sources of revenues on the web. Greater frequency of certain domains on the web indicates that they are comparatively more prominent than ot\n-hers which may also help them with more ad revenues as more prominent domains will be able to draw more web traffic to their site. Some of the domains contain specifc content which can help\nin targeted advertising. For example, \u0027fangraphs.com\u0027 shows content only about baseball and so it will attract people who are interested in the game and this fact could be used by  companies \nthat manufacture and sell baseball equipment to target the ads to only those people. When I visited \u0027osnews.com\u0027, I got two ads : one for \u0027Aruba\u0027 cloud solutions and another for Dell XPS lap\n-tops.\n\n\u003e *Domain Frequencies may help understand position on the web*\n\nDomain frequencies may help understand the current position of a particular domain on the web. Domains having more frequencies have better current positions on the web and hence are able  to\ndraw more traffic to their website in comparison to any other webiste that displays content on the same topic. More prominent domains will have more ad revenues therefore frequency  data  is\nan important tool of measuring a website\u0027s performance on the web. Once people working on a domain know the relative positions on the web, they can figure out what changes led to a  decrease\nor increase in the traffic to their site and compare their website presentaiton against those that have more or less frequencies enabling them to get a measure of themselves and enhancing th\n-eir chances of improvement. This can be noticed in the frequency data shown below, for the beginning segments, \u0027osnews.com\u0027 has been on top for news related to computer industry  while  for\nthe ending segments, \u0027dailytech.com\u0027 gained the top position in terms of the frequncies. Apart from other factors, this may be due recent changes in the websites and their presentation.",
      "dateUpdated": "Jul 17, 2016 5:05:40 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468725539925_-258982988",
      "id": "20160717-031859_373936478",
      "dateCreated": "Jul 17, 2016 3:18:59 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph generates the data needed for visualization\n\nval mayBegFreq \u003d mayBegData.keepValidPages()                        //get the domain frequency data for the beginning sections of May\n                           .map(r \u003d\u003e ExtractDomain(r.getUrl))\n                           .countItems()\n                           .take(60)\n                           \nval mayEndFreq \u003d mayEndData.keepValidPages()                        //get the domain frequency data for the ending sections of May\n                           .map(r \u003d\u003e ExtractDomain(r.getUrl))\n                           .countItems()\n                           .take(60)\n                           \nval mayBegFreqTbl \u003d sc.parallelize(mayBegFreq).toDF().registerTempTable(\"MayBegFreq\")\nval mayEndFreqTbl \u003d sc.parallelize(mayEndFreq).toDF().registerTempTable(\"MayEndFreq\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 17, 2016 4:01:49 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468725908911_-740179100",
      "id": "20160717-032508_711515394",
      "dateCreated": "Jul 17, 2016 3:25:08 AM",
      "dateStarted": "Jul 17, 2016 4:01:49 AM",
      "dateFinished": "Jul 17, 2016 4:05:49 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Domain Frequencies(May Beginning)",
      "text": "%sql\nselect MBF._1 as Domain, MBF._2 as Frequency from MayBegFreq MBF",
      "authenticationInfo": {},
      "dateUpdated": "Jul 17, 2016 4:14:26 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "Domain",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "Frequency",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "Domain",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "Frequency",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "helium": {},
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468726424620_-1887180157",
      "id": "20160717-033344_1077678638",
      "dateCreated": "Jul 17, 2016 3:33:44 AM",
      "dateStarted": "Jul 17, 2016 4:08:04 AM",
      "dateFinished": "Jul 17, 2016 4:08:05 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Domain Frequencies(May Ending)",
      "text": "%sql \nselect MEF._1 as Domain, MEF._2 as Frequency from MayEndFreq MEF",
      "authenticationInfo": {},
      "dateUpdated": "Jul 17, 2016 4:14:36 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "Domain",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "Frequency",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "Domain",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "Frequency",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "helium": {},
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468728484862_817482312",
      "id": "20160717-040804_2122775753",
      "dateCreated": "Jul 17, 2016 4:08:04 AM",
      "dateStarted": "Jul 17, 2016 4:11:22 AM",
      "dateFinished": "Jul 17, 2016 4:11:22 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 2. Analysis of the Site Link Structure\n[Image]\n\nThe analysis of site link structure would reveal important information about how the sites are linked and which websites benefit from being linked to others. The domain frequencies as indicated\nin the bar charts above suggest that few of the websites tend to be more frequent or prominent than the others and other domains may benefit from these prominent ones if they have  their  links\nplaced on the websites of the prominent domains, a phenomena on the web referred to as the **last click attribution model** which is finding out which link did the user click before landing  on\na particular page. This is specially profitable for those websites that have their links placed on the most prominent domains. The paragraph below generates the visualization  data  needed  for\nthe sankey diagrams which would best represent the outlinks from these more popularly used domains and the others that benefit from them. The paragraphs that follow present the sankey  diagrams\nthemselves.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 7:30:11 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468728682169_-1170455603",
      "id": "20160717-041122_311565590",
      "dateCreated": "Jul 17, 2016 4:11:22 AM",
      "dateStarted": "Jul 27, 2016 7:28:43 AM",
      "dateFinished": "Jul 27, 2016 7:28:43 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph attempts to generate the data needed for site link structure analysis (the outlinks from the various most prominent domains)\n\nimport StringUtils._\nimport org.apache.spark.rdd.RDD\nimport org.warcbase.spark.archive.io.ArchiveRecord\n\ndef getSankeyRep(inputDom : RDD[ArchiveRecord]) : Array[String] \u003d {                     //function to extract links from the crawl data for a particular domain\n    \n    inputDom.flatMap(r \u003d\u003e ExtractLinks(r.getUrl, r.getContentString))                   //perform the series of extractions and filtering\n            .map(r \u003d\u003e (ExtractDomain(r._1), ExtractDomain(r._2)))\n            .filter(r \u003d\u003e (r._1.compareTo(r._2) !\u003d 0))\n            .map(r \u003d\u003e (r, 1))\n            .reduceByKey((x, y) \u003d\u003e x + y)\n            .map(r \u003d\u003e \"[ \u0027\" + r._1._1 + \"\u0027, \u0027\" + r._1._2 + \"\u0027, \" + r._2 + \" ]\")\n            .take(10)\n}\n\nval mayBegLink1 \u003d mayBegData.keepValidPages()                                           //generate the data for visualization for only \u0027www.fangraphs.com\u0027\n                            .keepDomains(Set(\"www.fangraphs.com\"))\nval mayBegLnkWt1 \u003d \"[\" + getSankeyRep(mayBegLink1).mkString(\",\") + \"]\"                  //get the link data for \u0027fangraphs.com\u0027 in Sankey representation\n                            \nval mayBegLink2 \u003d mayBegData.keepValidPages()                                           //generate the data for visualization for only \u0027www.osnews.com\u0027\n                            .keepDomains(Set(\"www.osnews.com\"))\nval mayBegLnkWt2 \u003d \"[\" + getSankeyRep(mayBegLink2).mkString(\",\") + \"]\"                  //get the link data for \u0027osnews.com\u0027 in Sankey representation\n\nval mayBegLink3 \u003d mayBegData.keepValidPages()                                           //generate the data for visualization for only \u0027www.google.com\u0027\n                            .keepDomains(Set(\"www.google.com\"))\nval mayBegLnkWt3 \u003d \"[\" + getSankeyRep(mayBegLink3).mkString(\",\") + \"]\"                  //get the link data for \u0027google.com\u0027 in Sankey representation",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 2:28:19 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468737680086_101191989",
      "id": "20160717-064120_1940741597",
      "dateCreated": "Jul 17, 2016 6:41:20 AM",
      "dateStarted": "Jul 25, 2016 2:08:54 AM",
      "dateFinished": "Jul 25, 2016 2:17:08 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThe diagrams below present the outlinks from the various sites that have huge web traffic inflows. The other sites that are linked to the these famous sites take advantage of the\nphenomenon of **\u0027random browsing\u0027** by which a general user after staying on a website for some time will change the website that he/she is viewing. So users that are on these sites have a high\nprobability of changing to websites that contain their links on these pages. This probability of switching to a particular site increases with the number of times that a particular  webpage  is\nmentioned on a source-page. For example, if a \u0027source-page\u0027 contains 5 links to \u0027destination-page1\u0027 and 2 links to \u0027destination-page2\u0027 then there is a higher probability of a viewer landing  on\n\u0027destination-page1\u0027 than on \u0027destination-page2\u0027. These link frequencies have been measured as \u0027weights\u0027 and thickness of the links in the sankey diagrams is proportional to the weight if a link\non hovering the mouse over a particular link in the diagram, the exact weight value of the link is displayed. This helps us know about the websites that benefit the most out of the popular ones.\n\n\u003e *Sports site links more to other sports sites*\n\nThe sports site \u0027fangraphs.com\u0027 links more to other sports sites with the most frequent being \u0027hardballtimes.com\u0027 with a weight value of 500 which means that 500 mentions of the latter site was\nfound on the crawl data of \u0027fangraphs.com\u0027. Few other sites that contain their links on \u0027fangraphs.com\u0027 are \u0027foxsports.com\u0027 and \u0027baseballinfosolutions.com\u0027 which are also sports sites.  A  good\namount of the traffic of \u0027fangraphs.com\u0027 may get deviated to these other sites. Furthermore, realizing that the importance of the site \u0027fangraphs.com\u0027 a lot of other domains may be under compet\n-ition to have their names and links appear on the site \u0027fangraphs.com\u0027 and this brings advantage to the site itself as it may charge these other sites for keeping these names on its sites. Cha\nrges may even vary for the sites that want to have more mentions of their links on \u0027fangraphs.com\u0027. This gives a pretty good picture of the working of the web advertisement and website business\nmodel.\n\n\u003e *Technology site links more to other technology sites*\n\nThe technology site \u0027osnewws.com\u0027 links more to other technology sites and to other video sites for videos that are embedded in the website or links to other videos for product  information  or\nnews about product launches. \u0027theverge.com\u0027 is another site that contains information about technology although it is more general than \u0027osnews.com\u0027, the important fact is  that  \u0027theverge.com\u0027\ncontains the maximum links or mentions on \u0027osnews.com\u0027, followed by \u0027h-online.com\u0027 and the others, however the weight values are not as much as they had been in the previous cases and differ on\n-ly slightly but still the other sites do benefit by having their names mentioned on the \u0027osnews.com\u0027 and it may be a source of constant revenue for \u0027osnews.com\u0027 to put the names of these other\nsites on its webpages.\n\n\u003e *Google links only to Google*\n\nThe last of the sankey diagrams in this section present the outlinks for \u0027google.com\u0027, the most used internet search service. We know that search is one of the most used services on the interne\n-t and \u0027google\u0027 by far dominates this service. More and more people turn to \u0027google\u0027 for their search queries and millions of search queries are services every day which means that  the  search\nservice has an increasingly large number of visitors that are only monotonically increasing. To take advantage of this situation, as we find in the graphs below, the website links only  to  its\nown domains with the most dominant being \u0027scholar.google.com\u0027, a website for scholarly articles, this is followed by \u0027drive.google.com\u0027 and \u0027mapa.google.com\u0027, google\u0027s cloud storage and map ser\n-vices respectively. This sort of creates a positive feedback cycle and directs the viewers to these other services of \u0027google.com\u0027 with apparently the most used being \u0027scholar.google.com\u0027  and\nso the users keep surfing the web within the domain boundaries of \u0027google.com\u0027. They may change the domains manually by typing another address in the address bar or through bookmarks. Further,\nas we might have noticed \u0027google\u0027 displays advertisements as small frames embedded in its pages, so for users surfing around only on the domains of \u0027google.com\u0027 he will get to see more  ads  of\ncompanies that have paid \u0027google\u0027 for displaying their ads. Through browser tracking and other optimizations, these ads get tailored to specific viewers depending on their  particular  browsing\nhistory.\n\nFor advertisement being the most important source of revenues on the internet, we find that the content of the domains helps them get more web traffic. Also, web traffic specifically is more on\nsites that contain content regarding sports, technology, news and ofcourse search services - the requirement to find almost anything on the internet.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 4:57:17 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468914115060_-1806170053",
      "id": "20160719-074155_787667829",
      "dateCreated": "Jul 19, 2016 7:41:55 AM",
      "dateStarted": "Jul 25, 2016 2:21:21 AM",
      "dateFinished": "Jul 25, 2016 2:23:23 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Outlinks from \u0027fangraphs.com\u0027",
      "text": "//sankey representation for outlinks from \u0027fangraphs.com\u0027\n\nprintln(s\"\"\"%html\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cscript type\u003d\"text/javascript\" src\u003d\"https://www.gstatic.com/charts/loader.js\"\u003e\u003c/script\u003e\n    \n    \u003cscript type\u003d\"text/javascript\"\u003e\n      google.charts.load(\u0027current\u0027, {\u0027packages\u0027:[\u0027sankey\u0027]});\n      google.charts.setOnLoadCallback(drawChart);\n\n      function drawChart() {\n      \n        var data \u003d new google.visualization.DataTable();\n        data.addColumn(\u0027string\u0027, \u0027From\u0027);\n        data.addColumn(\u0027string\u0027, \u0027To\u0027);\n        data.addColumn(\u0027number\u0027, \u0027Weight\u0027);\n        data.addRows($mayBegLnkWt1);\n\n        var colors \u003d [\u0027#a6cee3\u0027, \u0027#b2df8a\u0027, \u0027#fb9a99\u0027, \u0027#fdbf6f\u0027,\n                  \u0027#cab2d6\u0027, \u0027#ffff99\u0027, \u0027#1f78b4\u0027, \u0027#33a02c\u0027];\n\n        var options \u003d {\n          height: 600,\n          sankey: {\n            node: {\n              colors: colors\n            },\n            link: {\n              colorMode: \u0027gradient\u0027,\n              colors: colors\n            }\n          }\n        };\n\n        // Instantiates and draws our chart, passing in some options.\n        var chart \u003d new google.visualization.Sankey(document.getElementById(\u0027sankey1\u0027));\n        chart.draw(data, options);\n      }\n    \u003c/script\u003e\n  \u003c/head\u003e\n      \n  \u003cdiv id\u003d\"sankey1\" style\u003d\"width: 500px; height: 600px;\"\u003e\u003c/div\u003e\n\u003c/html\u003e\n\"\"\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 3:47:35 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468738973315_-262354159",
      "id": "20160717-070253_585580025",
      "dateCreated": "Jul 17, 2016 7:02:53 AM",
      "dateStarted": "Jul 25, 2016 3:47:36 AM",
      "dateFinished": "Jul 25, 2016 3:47:36 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Outlinks from \u0027osnews.com\u0027",
      "text": "//sankey representation for outlinks from \u0027osnews.com\u0027\n\nprintln(s\"\"\"%html\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cscript type\u003d\"text/javascript\" src\u003d\"https://www.gstatic.com/charts/loader.js\"\u003e\u003c/script\u003e\n  \n    \u003cscript type\u003d\"text/javascript\"\u003e\n      google.charts.load(\u0027current\u0027, {\u0027packages\u0027:[\u0027sankey\u0027]});\n      google.charts.setOnLoadCallback(drawChart);\n\n      function drawChart() {\n      \n        var data \u003d new google.visualization.DataTable();\n        data.addColumn(\u0027string\u0027, \u0027From\u0027);\n        data.addColumn(\u0027string\u0027, \u0027To\u0027);\n        data.addColumn(\u0027number\u0027, \u0027Weight\u0027);\n        data.addRows($mayBegLnkWt2);\n\n        var colors \u003d [\u0027#a6cee3\u0027, \u0027#b2df8a\u0027, \u0027#fb9a99\u0027, \u0027#fdbf6f\u0027,\n                  \u0027#cab2d6\u0027, \u0027#ffff99\u0027, \u0027#1f78b4\u0027, \u0027#33a02c\u0027];\n\n        var options \u003d {\n          height: 600,\n          sankey: {\n            node: {\n              colors: colors\n            },\n            link: {\n              colorMode: \u0027gradient\u0027,\n              colors: colors\n            }\n          }\n        };\n\n        // Instantiates and draws our chart, passing in some options.\n        var chart \u003d new google.visualization.Sankey(document.getElementById(\u0027sankey2\u0027));\n        chart.draw(data, options);\n      }\n    \u003c/script\u003e\n  \u003c/head\u003e\n  \n  \u003cdiv id\u003d\"sankey2\" style\u003d\"width: 500px; height: 600px;\"\u003e\u003c/div\u003e\n\u003c/html\u003e\n\"\"\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 4:09:04 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468835221756_1050565259",
      "id": "20160718-094701_299717493",
      "dateCreated": "Jul 18, 2016 9:47:01 AM",
      "dateStarted": "Jul 25, 2016 4:09:04 AM",
      "dateFinished": "Jul 25, 2016 4:09:04 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Outlinks from \u0027google.com\u0027",
      "text": "//sankey representation for outlinks from \u0027google.com\u0027\n\nprintln(s\"\"\"%html\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cscript type\u003d\"text/javascript\" src\u003d\"https://www.gstatic.com/charts/loader.js\"\u003e\u003c/script\u003e\n  \n    \u003cscript type\u003d\"text/javascript\"\u003e\n      google.charts.load(\u0027current\u0027, {\u0027packages\u0027:[\u0027sankey\u0027]});\n      google.charts.setOnLoadCallback(drawChart);\n\n      function drawChart() {\n      \n        var data \u003d new google.visualization.DataTable();\n        data.addColumn(\u0027string\u0027, \u0027From\u0027);\n        data.addColumn(\u0027string\u0027, \u0027To\u0027);\n        data.addColumn(\u0027number\u0027, \u0027Weight\u0027);\n        data.addRows($mayBegLnkWt3);\n\n        var colors \u003d [\u0027#a6cee3\u0027, \u0027#b2df8a\u0027, \u0027#fb9a99\u0027, \u0027#fdbf6f\u0027,\n                  \u0027#cab2d6\u0027, \u0027#ffff99\u0027, \u0027#1f78b4\u0027, \u0027#33a02c\u0027];\n\n        var options \u003d {\n          height: 800,\n          width: 900,\n          sankey: {\n            node: {\n              colors: colors\n            },\n            link: {\n              colorMode: \u0027gradient\u0027,\n              colors: colors\n            }\n          }\n        };\n\n        // Instantiates and draws our chart, passing in some options.\n        var chart \u003d new google.visualization.Sankey(document.getElementById(\u0027sankey3\u0027));\n        chart.draw(data, options);\n      }\n    \u003c/script\u003e\n  \u003c/head\u003e\n  \n  \u003cdiv id\u003d\"sankey3\" style\u003d\"width: 900px; height: 800px;\"\u003e\u003c/div\u003e\n\u003c/html\u003e\n\"\"\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 1:01:29 PM",
      "config": {
        "colWidth": 9.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468740754704_1620918899",
      "id": "20160717-073234_318503326",
      "dateCreated": "Jul 17, 2016 7:32:34 AM",
      "dateStarted": "Jul 25, 2016 1:01:29 PM",
      "dateFinished": "Jul 25, 2016 1:01:30 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Fact File",
      "text": "%md \n![Doodle](http://www.google.com/logos/doodles/2016/holi-festival-2016-4796769118453760-hp2x.gif)\n\n1. Google takes over 200 factors into account before delivering the correct search results for any query in a fraction of a second.\n\n2. The company owns a bunch of domains that are common misspellings of \u0027Google\u0027 like \u0027Gooogle.com\u0027, \u0027Gogle.com\u0027, \u0027Googlr.com\u0027 and more.\n\n3. There are more than 2 million googles searches per second.\n\n4. In 2013, 91% of the revenues of \u0027Google.com\u0027 came from advertising alone.\n\n5. Google\u0027s search index is more than 100 million gigabytes in size. It would take 100,000 one-terabyte personal drives to contain the same amount of data.\n\n6. Google might be the only company with the explicit goal to REDUCE the amount of time people spend on its site. \n\n7. Google has photographed more than 5 million miles of road for its Street View maps.\n\n8. Google has acquired 24 companies this year alone â€” that\u0027s about three companies a month. ",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2016 5:20:27 PM",
      "config": {
        "colWidth": 3.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468845555820_1768338820",
      "id": "20160718-123915_1536352968",
      "dateCreated": "Jul 18, 2016 12:39:15 PM",
      "dateStarted": "Jul 25, 2016 5:20:28 PM",
      "dateFinished": "Jul 25, 2016 5:20:28 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 3. Analysis of the Web Graph structure using GraphX\n[Image]\n\nThis section examines the web graph structure of the web using the common crawl data and the GraphX libraries of Spark. We know that in random browsing, the probability that a web user/viewer\nwill land on a particular page is determined by the \u0027page rank\u0027 value of the page. Using the links contained in the pages of the crawl data, we first calculate the \u0027page rank\u0027 value of the pa\n-ges. This gives the size value of the nodes that represent the individual pages and the links that they have to other pages. Visualization for the graph is accomplished using D3 libraries.\n\nThe paragraph below first generates the data needed for the visualization.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 7:30:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1468845771609_-1196027218",
      "id": "20160718-124251_1535643421",
      "dateCreated": "Jul 18, 2016 12:42:51 PM",
      "dateStarted": "Jul 27, 2016 7:30:15 AM",
      "dateFinished": "Jul 27, 2016 7:30:15 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph first generates the data for the visualization by extractign the link graph from the crawl data using GraphX\n\nval rawRecord \u003d mayBegData.keepValidPages().keepDomains(Set(\"www.fangraphs.com\", \"www.osnews.com\", \"www.google.com\"))       //extract only the domains needed to get smaller data\n\nval graph \u003d ExtractGraph(rawRecord, dynamic\u003dtrue)       //extract the graph from the data\ngraph.writeAsJson(\"/home/ubuntu/Documents/LinkGraphics/data/nodes\", \"/home/ubuntu/Documents/LinkGraphics/data/links\")       //write the data to disk to form the graph.json input file",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 3:27:38 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469508848154_1556987810",
      "id": "20160726-045408_104185919",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "rawRecord: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[23] at filter at RecordRDD.scala:82\ngraph: org.apache.spark.graphx.Graph[org.warcbase.spark.matchbox.ExtractGraph.VertexData,org.warcbase.spark.matchbox.ExtractGraph.EdgeData] \u003d org.apache.spark.graphx.impl.GraphImpl@5e7bb67\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ubuntu/Documents/LinkGraphics/data/links already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n\tat org.warcbase.spark.matchbox.ExtractGraph$GraphWriter.writeAsJson(ExtractGraph.scala:87)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:53)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:55)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:57)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:59)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat \u003cinit\u003e(\u003cconsole\u003e:75)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:813)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:759)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:752)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:382)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Jul 26, 2016 4:54:08 AM",
      "dateStarted": "Jul 27, 2016 3:27:38 AM",
      "dateFinished": "Jul 27, 2016 3:29:53 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nTo better understand the web structure of the internet, we have a visualization of the web graph structure shown below. The nodes represent the webpages and the links represent the\nlinks between the webpages. The size of the nodes is proportional to their \u0027page rank\u0027 values which in turn is calculated from the page ranking algorithm after considering the total inlinks\nand outlinks. \n\n\u003e *Page Rank : An indicator of the relative importance of the web sites*\n\nBy the size of the nodes, we may well be able to visually estimate the probability that any random user when surfing on the web will land on a particular page. For example, for users surfin\n-g on sports sites, we can estimate that their probability of landing on the site \u0027fangraphs.com\u0027 will be higher than the rest of the sites. Similarly, for sites of other subjects. This  is\nthe main idea behind the results returned by search engines - the results are displayed in the order of the decreasing page rank values of the pages. This fact is also confirmed by the doma\n-in frequency values calculated above, \u0027fangraphs.com\u0027 had the highest domain frequencies among all the others and here too it has the highest page rank value among all the nodes displayed.\nAdditionally, pages that are linked to these pages with a higher page rank value have the benefit of being visited by the visitors of the previous page.\n\n\u003e *Authorities and Hubs*\n\nThe node structure can better be explained by considering the following classification of the nodes : Authorities and Hubs. \u0027Authorities\u0027 refers to nodes that are linked by others and \u0027Hubs\u0027\nrefers to websites dealing with a lot of links. For \u0027Authorities\u0027, the more the number of inlinks, the bigger will be the nodes(more authority) which is also relected in its page rank value \nFor the visualization displayed below, we can find that all the bigger nodes that contain large number of links to other sites, namely \u0027fangraphs.com\u0027, \u0027osnews.com\u0027 and \u0027google.com\u0027 are \u0027Hu\n-bs\u0027 while the other sites that are linked to by other sites are \u0027Authorities\u0027. If the visualisation is taken to be a very small sub-space of the whole web, then we find that the \u0027Hubs\u0027 ten\n-d to dominate that space and draw more web traffic than \u0027Authorities\u0027. Further, \u0027Authorities\u0027 rely on the \u0027Hubs\u0027 for a part of the inflow of traffic to their websites which in turn facilit\n-ates advertisement revenues. This is easily visible in the graph structure shown below, \u0027hardballtimes.com\u0027 has more authority than any other site linked by \u0027fangraphs.com\u0027.\n\n\u003e *Edges of the graph are indicative of the strength of links*\n\nThe links between websites create \"attraction\" to make them appear closer to each other. A good way to test this is to pin \u0027google.com\u0027 and \u0027osnews.com\u0027 far apart from each other  and  then\nrelease them. The two domains will be pulled towards each other quite visibly which demonstrates the strength of the links between the websites. This is valid for all other websites too (if\nthey are linked). Moreover, the \u0027domains\u0027 that have more page rank values (bigger size) clutter around the bigger \u0027Hubs\u0027 and they tend to form clusters of communities in the web  space.  In\nthe sample web space seen below, we can easily notice the communities that are formed around \u0027fangraphs.com\u0027, \u0027osnews.com\u0027 and \u0027google.com\u0027. To test this further, one can just  move  freely\nany of these bigger \u0027hubs\u0027 and the websites that form the communities, move around along with them as is they were glued by some force which demonstrates the effect being described. The rel\n-ative strength of links is also indicated by the count of links which can be found by hovering the mouse over an edge of the graph.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 7:28:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469509547401_1200887117",
      "id": "20160726-050547_1843529404",
      "dateCreated": "Jul 26, 2016 5:05:47 AM",
      "dateStarted": "Jul 27, 2016 7:28:18 AM",
      "dateFinished": "Jul 27, 2016 7:28:18 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph generates the visualization needed for web structure analysis\n\nprintln(\"\"\"%html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cmeta charset\u003d\"utf-8\" name\u003d\"viewport\" content\u003d\"width\u003ddevice-width, initial-scale\u003d1\"\u003e\n    \u003clink rel\u003d\"shortcut icon\" href\u003d\"data:image/x-icon\" type\u003d\"image/x-icon\"\u003e\n    \u003c!-- noUiSlider --\u003e\n    \u003clink rel\u003d\"stylesheet\" href\u003d\"linkassets/css/lib/nouislider.min.css\"\u003e\n    \u003c!-- App --\u003e\n    \u003clink href\u003d\"linkassets/css/app.css\" rel\u003d\"stylesheet\"\u003e\n  \u003c/head\u003e\n\n    \u003cdiv class\u003d\"container-fluid\"\u003e\n      \u003cheader\u003e\n        \u003cdiv class\u003d\"page-header\"\u003e\n          \u003ch1\u003eVisualization of the Web Graph Structure\u003c/h1\u003e\n        \u003c/div\u003e\n      \u003c/header\u003e\n\n      \u003csection\u003e\n        \u003cdiv class\u003d\"row sliders\"\u003e\n          \u003cdiv class\u003d\"col-md-6\"\u003e\n            \u003clabel for\u003d\"threshold-nodes\"\u003eMax sites to display\u003c/label\u003e\n            \u003cdiv id\u003d\"threshold-slider\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n\t\u003c!--\n          \u003cdiv class\u003d\"col-md-6\"\u003e\n            \u003clabel for\u003d\"threshold-nodes\"\u003eYear\u003c/label\u003e\n            \u003cdiv id\u003d\"date-slider\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n\t--\u003e\n        \u003c/div\u003e\n        \u003cdiv class\u003d\"row\"\u003e\n          \u003cdiv id\u003d\"display\" class\u003d\"col-md-12\"\u003e\n            \u003cdiv id\u003d\"graph\" class\u003d\"d3-graph\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/section\u003e\n    \u003c/div\u003e\n    \u003c!-- jQuery --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/jquery.isloading.min.js\"\u003e\u003c/script\u003e\n    \u003c!-- Bootstrap --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/d3.tip.v0.6.3.js\"\u003e\u003c/script\u003e\n    \u003c!-- noUiSlider --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/nouislider.min.js\"\u003e\u003c/script\u003e\n    \u003c!-- App --\u003e\n    \u003cscript src\u003d\"/linkassets/js/app.js\"\u003e\u003c/script\u003e\n\u003c/html\u003e\n\"\"\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 26, 2016 4:51:44 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469449441162_-779669184",
      "id": "20160725-122401_296434994",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cmeta charset\u003d\"utf-8\" name\u003d\"viewport\" content\u003d\"width\u003ddevice-width, initial-scale\u003d1\"\u003e\n    \u003clink rel\u003d\"shortcut icon\" href\u003d\"data:image/x-icon\" type\u003d\"image/x-icon\"\u003e\n    \u003c!-- noUiSlider --\u003e\n    \u003clink rel\u003d\"stylesheet\" href\u003d\"linkassets/css/lib/nouislider.min.css\"\u003e\n    \u003c!-- App --\u003e\n    \u003clink href\u003d\"linkassets/css/app.css\" rel\u003d\"stylesheet\"\u003e\n  \u003c/head\u003e\n\n    \u003cdiv class\u003d\"container-fluid\"\u003e\n      \u003cheader\u003e\n        \u003cdiv class\u003d\"page-header\"\u003e\n          \u003ch1\u003eVisualization of the Web Graph Structure\u003c/h1\u003e\n        \u003c/div\u003e\n      \u003c/header\u003e\n\n      \u003csection\u003e\n        \u003cdiv class\u003d\"row sliders\"\u003e\n          \u003cdiv class\u003d\"col-md-6\"\u003e\n            \u003clabel for\u003d\"threshold-nodes\"\u003eMax sites to display\u003c/label\u003e\n            \u003cdiv id\u003d\"threshold-slider\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n\t\u003c!--\n          \u003cdiv class\u003d\"col-md-6\"\u003e\n            \u003clabel for\u003d\"threshold-nodes\"\u003eYear\u003c/label\u003e\n            \u003cdiv id\u003d\"date-slider\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n\t--\u003e\n        \u003c/div\u003e\n        \u003cdiv class\u003d\"row\"\u003e\n          \u003cdiv id\u003d\"display\" class\u003d\"col-md-12\"\u003e\n            \u003cdiv id\u003d\"graph\" class\u003d\"d3-graph\"\u003e\u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/section\u003e\n    \u003c/div\u003e\n    \u003c!-- jQuery --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/jquery.isloading.min.js\"\u003e\u003c/script\u003e\n    \u003c!-- Bootstrap --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/d3.tip.v0.6.3.js\"\u003e\u003c/script\u003e\n    \u003c!-- noUiSlider --\u003e\n    \u003cscript src\u003d\"/linkassets/js/lib/nouislider.min.js\"\u003e\u003c/script\u003e\n    \u003c!-- App --\u003e\n    \u003cscript src\u003d\"/linkassets/js/app.js\"\u003e\u003c/script\u003e\n\u003c/html\u003e\n\n"
      },
      "dateCreated": "Jul 25, 2016 12:24:01 PM",
      "dateStarted": "Jul 26, 2016 4:50:10 AM",
      "dateFinished": "Jul 26, 2016 4:50:10 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 4. Measuring the impact of Google Analytics\n![GoogleAnalytics](https://livecode.com/wp-content/uploads/2016/05/connector-google-analytics-logo-300x138.png)\n\nGoogle Analytics is by far the most preferred solution among website hosting companies to measure the inflow of web traffic to their sites. This section of the notebook measures the impact    \u003cbr/\u003e\nof google analytics on the web by considering the sample space of the data for the first three segments of May. Although this may not be completely sufficient to give exact and accurate re    \u003cbr/\u003e\n-sults but it will let us have an idea of the impact of google analytics. The strategy is simple and involves firstly finding the percentage of webpages that have google analytics  enabled    \u003cbr/\u003e\non their sites. Then, we proceed to calculate the leaked browser history, as given by Stephen Merity\u0027s simple algorithm :\n\n\u003cbr/\u003e\n\u003e*Algorithm for the leaked browsing history(for Google Analytics)*\n\n            for(each link \u003d {pageA} -\u003e {pageB})\n                totalLinks +\u003d 1\n                if({pageA} or {pageB} has Google Analytics)\n                    totalLeaked +\u003d 1\n        Leaked Browser History \u003d totalLeaked/totalLinks\n\n\u003cbr/\u003e\nSince the number of pages(crawl data) is a subset of the whole common crawl corpus, we expect to find higher number of pages that have google analytics enabled on them. Nonetheless, we proc   \u003cbr/\u003e\n-eed to calculate the above quantities and try to draw conclusions based on the results displayed in the paragraph below. About 54.63% of the total number of pages have google analytics ena   \u003cbr/\u003e\n-bled which is about half the total number of pages. Next, we use the graph extracted in the above section to get the number of edges(or links) for the domains under consideration; the grap   \u003cbr/\u003e\n-h is first trimmed to remove the duplicate edges and then the number of links is counted. A sample of the links RDD is displayed in the results. All the links have their source domains  as   \u003cbr/\u003e\neither of \u0027fangraphs.com\u0027, \u0027osnews.com\u0027 or \u0027google.com\u0027. From the filtered data for these domains, we find out that all of these domains have google analytics enabled and hence by the above   \u003cbr/\u003e\nalgorithm, totalLeaked \u003d 172 which is the same as totalLinks and hence 100% of the browsing history is leaked to Google for this sample set of wenb crawl data. This is much more than the re   \u003cbr/\u003e\n-sults obtained by Stephen Merity in his analysis but the difference can be attributed to the scale of data being considered in the two cases.",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 7:35:59 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469451068144_-2033055218",
      "id": "20160725-125108_487895136",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e4. Measuring the impact of Google Analytics\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://livecode.com/wp-content/uploads/2016/05/connector-google-analytics-logo-300x138.png\" alt\u003d\"GoogleAnalytics\" /\u003e\u003c/p\u003e\n\u003cp\u003eGoogle Analytics is by far the most preferred solution among website hosting companies to measure the inflow of web traffic to their sites. This section of the notebook measures the impact    \u003cbr /\u003e\n\u003cbr  /\u003eof google analytics on the web by considering the sample space of the data for the first three segments of May. Although this may not be completely sufficient to give exact and accurate re    \u003cbr /\u003e\n\u003cbr  /\u003e-sults but it will let us have an idea of the impact of google analytics. The strategy is simple and involves firstly finding the percentage of webpages that have google analytics  enabled    \u003cbr /\u003e\n\u003cbr  /\u003eon their sites. Then, we proceed to calculate the leaked browser history, as given by Stephen Merity\u0027s simple algorithm :\u003c/p\u003e\n\u003cp\u003e\u003cbr /\u003e\u003c/p\u003e\n\u003cblockquote\u003e\u003cp\u003e\u003cem\u003eAlgorithm for the leaked browsing history(for Google Analytics)\u003c/em\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e        for(each link \u003d {pageA} -\u0026gt; {pageB})\n            totalLinks +\u003d 1\n            if({pageA} or {pageB} has Google Analytics)\n                totalLeaked +\u003d 1\n    Leaked Browser History \u003d totalLeaked/totalLinks\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cbr /\u003e\n\u003cbr  /\u003eSince the number of pages(crawl data) is a subset of the whole common crawl corpus, we expect to find higher number of pages that have google analytics enabled on them. Nonetheless, we proc   \u003cbr /\u003e\n\u003cbr  /\u003e-eed to calculate the above quantities and try to draw conclusions based on the results displayed in the paragraph below. About 54.63% of the total number of pages have google analytics ena   \u003cbr /\u003e\n\u003cbr  /\u003e-bled which is about half the total number of pages. Next, we use the graph extracted in the above section to get the number of edges(or links) for the domains under consideration; the grap   \u003cbr /\u003e\n\u003cbr  /\u003e-h is first trimmed to remove the duplicate edges and then the number of links is counted. A sample of the links RDD is displayed in the results. All the links have their source domains  as   \u003cbr /\u003e\n\u003cbr  /\u003eeither of \u0027fangraphs.com\u0027, \u0027osnews.com\u0027 or \u0027google.com\u0027. From the filtered data for these domains, we find out that all of these domains have google analytics enabled and hence by the above   \u003cbr /\u003e\n\u003cbr  /\u003ealgorithm, totalLeaked \u003d 172 which is the same as totalLinks and hence 100% of the browsing history is leaked to Google for this sample set of wenb crawl data. This is much more than the re   \u003cbr /\u003e\n\u003cbr  /\u003e-sults obtained by Stephen Merity in his analysis but the difference can be attributed to the scale of data being considered in the two cases.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 25, 2016 12:51:08 PM",
      "dateStarted": "Jul 27, 2016 7:31:44 AM",
      "dateFinished": "Jul 27, 2016 7:31:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph tests the links with Google Analytics in it\nimport org.apache.spark.graphx.Graph\n\n//first calculate the total paercentage of pages that have GA enabled\nval totalPages \u003d mayBegData.keepValidPages()                                    //simply get the total number of pages\nval pagesWithGA \u003d totalPages.keepContent(Set(\"google-analytics\".r))             //get the number of pages with GA enabled\n\nval percentWithGA \u003d (pagesWithGA.count() * 1.0)/totalPages.count()                                      //percentage of pages with GA enabled on them\n\n//find the total leaked history parameters\ndef merge(edge1 : ExtractGraph.EdgeData, edge2 : ExtractGraph.EdgeData) : ExtractGraph.EdgeData \u003d edge1                  //define the merger for similar links\n\nval trimGraph \u003d graph.groupEdges((ED1, ED2) \u003d\u003e merge(ED1, ED2))\nval links \u003d trimGraph.edges                                                                                              //get the edges of the graph\n\nlinks.count()\nlinks.take(10).foreach(println)\n\nval domainWithGA \u003d rawRecord.keepContent(Set(\"google-analytics\".r))                                                      //get the pages that have google-analytics\nval distinctDomWithGA \u003d domainWithGA.map(r \u003d\u003e ExtractDomain(r.getUrl)).distinct()                                        //get the distinct domains with GA\ndistinctDomWithGA.count()\ndistinctDomWithGA.take(3).foreach(println)\n",
      "authenticationInfo": {},
      "dateUpdated": "Jul 27, 2016 6:58:34 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469588341186_-599194795",
      "id": "20160727-025901_2072319674",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.graphx.Graph\ntotalPages: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[422] at filter at RecordRDD.scala:52\npagesWithGA: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[423] at filter at RecordRDD.scala:90\npercentWithGA: Double \u003d 0.5463707137638255\nmerge: (edge1: org.warcbase.spark.matchbox.ExtractGraph.EdgeData, edge2: org.warcbase.spark.matchbox.ExtractGraph.EdgeData)org.warcbase.spark.matchbox.ExtractGraph.EdgeData\ntrimGraph: org.apache.spark.graphx.Graph[org.warcbase.spark.matchbox.ExtractGraph.VertexData,org.warcbase.spark.matchbox.ExtractGraph.EdgeData] \u003d org.apache.spark.graphx.impl.GraphImpl@6f5d400e\nlinks: org.apache.spark.graphx.EdgeRDD[org.warcbase.spark.matchbox.ExtractGraph.EdgeData] \u003d EdgeRDDImpl[425] at RDD at EdgeRDD.scala:40\nres32: Long \u003d 172\nEdge(-1536293812,-1804144556,EdgeData(20160524,google.com,scholar.google.com))\nEdge(-1536293812,-1536293812,EdgeData(20160524,google.com,google.com))\nEdge(-1536293812,-78033866,EdgeData(20160524,google.com,youtube.com))\nEdge(-1536293812,28462918,EdgeData(20160524,google.com,play.google.com))\nEdge(-1536293812,186378243,EdgeData(20160524,google.com,maps.google.com))\nEdge(-1536293812,598282032,EdgeData(20160524,google.com,drive.google.com))\nEdge(-1536293812,847444939,EdgeData(20160524,google.com,support.google.com))\nEdge(-1536293812,1336620259,EdgeData(20160524,google.com,mail.google.com))\nEdge(-1536293812,1912841799,EdgeData(20160524,google.com,news.google.com))\nEdge(-643285558,-2095271699,EdgeData(20160524,osnews.com,apple.com))\ndomainWithGA: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d MapPartitionsRDD[427] at filter at RecordRDD.scala:90\ndistinctDomWithGA: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[431] at distinct at \u003cconsole\u003e:76\nres34: Long \u003d 3\nwww.osnews.com\nwww.google.com\nwww.fangraphs.com\n"
      },
      "dateCreated": "Jul 27, 2016 2:59:01 AM",
      "dateStarted": "Jul 27, 2016 6:58:34 AM",
      "dateFinished": "Jul 27, 2016 7:05:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 5. Analysing the context of Locations\nThe common crawl data has WET format files that contain only the text data extracted from the crawled content, in this section we use that text data to analyse the context of locations mentioned\nin the data. The analysis is performed on the smaple data of the first segment of May to present a simple demostration of the location context search service described by the work done  on  this\nsubject by Vikas Bansal, Primal Pappachan and Abhishek Sethi. The simple procedure described by them is outlined below :-\n    \n\u003e    1. Analyse the web crawl data.\n     2. Create a list of locations that nearly covers the possible locations that may be present in the crawl data.\n     3. Filter top 100 words in the from the documents that mention a location.\n     4. Use an algorithm (such as Latent Dirichlet Allocation) to decipher topics from the data by using sufficient number of clusters.\n     5. Build an index of locations against the list of words/topics associated with that location.\n     6. Display the simple search service in the form of an interface.\n\nThe paragraph below first tries to create an index of the locations and their context by choosing the words closest to the location in the text data. The subsequent paragraph displays the subset\nof keys available and the interface for the search.",
      "authenticationInfo": {},
      "dateUpdated": "Aug 2, 2016 3:18:18 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469590355445_-935616407",
      "id": "20160727-033235_1724010516",
      "dateCreated": "Jul 27, 2016 3:32:35 AM",
      "dateStarted": "Jul 27, 2016 6:18:04 AM",
      "dateFinished": "Jul 27, 2016 6:22:53 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph creates the index needed for the context of locations\nimport org.apache.spark.rdd.RDD\nimport scala.collection.mutable.HashMap\n\ndef getRangeValues(index : Long) : Array[Long] \u003d {                                                  //function to get the range of values associated with a location\n    \n    val indxArr \u003d new Array[Long](20)\n    for(i \u003c- -9 until 10){\n        indxArr(i + 9) \u003d index + i\n    }\n    return indxArr\n}\n\nval rawLocations \u003d sc.textFile(\"/home/ubuntu/ZeppelinData/CommonCrawl/places.txt\")                  //locations read from an exhaustive list of places - places.txt\nval locations \u003d rawLocations.flatMap(ele \u003d\u003e ele.split(\" \")).map( _.toLowerCase).distinct()\n\nval mayWetSeg1 \u003d sc.textFile(\"/home/ubuntu/ZeppelinData/CommonCrawl/MayWetSeg1.warc.wet.gz\")        //the WET segments of May\n\nval textSeg1 \u003d mayWetSeg1.map(ele \u003d\u003e ele.replace(\".\", \"\") .replace(\",\", \"\") .replace(\"?\", \"\")       //first remove all the punctuation marks \n                                        .replace(\"/\", \"\") .replace(\":\", \"\") .replace(\"!\", \"\") \n                                        .replace(\";\", \"\") .replace(\"\u0027\", \"\") .replace(\"*\", \"\")\n                                        .replace(\"\\\"\", \"\")\n                                        \n                                        .replace(\" for \", \" \").replace(\" to \", \" \") .replace(\" I \", \" \")        //remove unnecessary stop words\n                                        .replace(\" the \", \" \") .replace(\" an \", \" \") .replace(\" a \", \" \")\n                                        .replace(\" The \", \" \") .replace(\" An \", \" \") .replace(\" A \", \" \")\n                                        .replace(\" we \", \" \") .replace(\" in \", \" \") .replace(\" on \", \" \")\n                                        .replace(\" be \", \" \") .replace(\" is \", \" \") .replace(\" are \", \" \")\n                                        .replace(\" of \", \" \") .replace(\" or \", \" \") .replace(\" and \", \" \")\n                                        .replace(\" it \", \" \") .replace(\" if \", \" \") .replace(\" can \", \" \")\n                                        .replace(\" was \", \" \").replace(\" has \", \" \").replace(\" had \", \" \")\n                                        .replace(\" been \", \" \").replace(\" were \", \" \").replace(\" have \", \" \")\n                                        .replace(\" they \", \" \").replace(\" that \", \" \").replace(\" this \", \" \"))\n                         .flatMap( _.split(\" \"))\n                         .map( _.toLowerCase)\n                         \nval locInTextSeg1 \u003d textSeg1.intersection(locations)                                               //get the locations present in the text data\nval locationsArr \u003d locInTextSeg1.collect()\n\nval indexedText \u003d textSeg1.zipWithIndex                                                            //create indexed collection of text words to be easily able to retrieve them\nval indexedLocs \u003d indexedText.filter(ele \u003d\u003e locationsArr.contains(ele._1))                         //get the locations present in text segments\nval validIndexs \u003d indexedLocs.filter(ele \u003d\u003e !(ele._1.equals(\"\")))                                  //get the valid indexes after removing the blanks and null strings\n\nval indxRange \u003d validIndexs.map(ele \u003d\u003e (ele._1, getRangeValues(ele._2))).take(1000)                //take the first thousand values as a sample\nval cntxtMap \u003d new HashMap[String, RDD[String]]                                                    //the HashMap to store the index of locations and their context\n\nfor(i \u003c- 0 until 1000){\n    cntxtMap.put(indxRange(i)._1, indexedText.filter(ele \u003d\u003e indxRange(i)._2.contains(ele._2)).map( _._1))       //create the map of indexes\n}",
      "authenticationInfo": {},
      "dateUpdated": "Aug 2, 2016 2:24:34 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469592083995_1508022904",
      "id": "20160727-040123_1246632602",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.rdd.RDD\nimport scala.collection.mutable.HashMap\ngetRangeValues: (index: Long)Array[Long]\nrawLocations: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[1] at textFile at \u003cconsole\u003e:31\nlocations: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[6] at distinct at \u003cconsole\u003e:33\nmayWetSeg1: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[8] at textFile at \u003cconsole\u003e:31\ntextSeg1: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[11] at map at \u003cconsole\u003e:49\nlocInTextSeg1: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[17] at intersection at \u003cconsole\u003e:39\nlocationsArr: Array[String] \u003d Array(denver, sydney, denton, manchester, arabia, luxor, francisco, bogota, vegas, glasgow, lousiana, iran, paul, yuma, bolivia, montreal, arlington, springs, edinburgh, anchorage, georgia, manila, johannesburg, lexington, columbus, tuscon, topeka, richmond, leipzig, allentown, turkey, glendale, baton, minnesota, marseille, birmingham, budapest, wilmington, palm, peru, dakota, oregon, paraguay, uruguay, indiana, omaha, hong, janerio, malasia, lublin, austin, korea, tennessee, lakeland, michigan, mcallen, jefferson, honolulu, florence, prague, amsterdam, salt, finland, rio, las, downey, ana, jersey, orleans, dublin, heights, poland, louisville, park, leicester, louis, plaines, buenos, israel, boise, kent, copenhagen, beijing, norway, venice, savannah, arizon...indexedText: org.apache.spark.rdd.RDD[(String, Long)] \u003d ZippedWithIndexRDD[18] at zipWithIndex at \u003cconsole\u003e:35\nindexedLocs: org.apache.spark.rdd.RDD[(String, Long)] \u003d MapPartitionsRDD[19] at filter at \u003cconsole\u003e:45\nvalidIndexs: org.apache.spark.rdd.RDD[(String, Long)] \u003d MapPartitionsRDD[20] at filter at \u003cconsole\u003e:47\nindxRange: Array[(String, Array[Long])] \u003d Array((kingdom,Array(1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 0)), (new,Array(1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 0)), (new,Array(1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 0)), (baltimore,Array(1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 0)), (little,Array(1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 0)), (new,Array(1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 194...cntxtMap: scala.collection.mutable.HashMap[String,org.apache.spark.rdd.RDD[String]] \u003d Map()\n"
      },
      "dateCreated": "Jul 27, 2016 4:01:23 AM",
      "dateStarted": "Aug 2, 2016 2:24:34 AM",
      "dateFinished": "Aug 2, 2016 2:29:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val keys \u003d cntxtMap.keySet                          //get to know the keys of the index to be able to try finding the context of locations\nprintln(keys)",
      "authenticationInfo": {},
      "dateUpdated": "Aug 2, 2016 2:31:49 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469592418881_530602522",
      "id": "20160727-040658_1211999994",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "keys: scala.collection.Set[String] \u003d Set(china, paraguay, minnesota, tallahassee, diego, town, nashville, philadelphia, angeles, germany, iowa, nevada, illinois, massachusetts, europe, kingdom, connecticut, colorado, island, district, city, delhi, dakota, boston, tennessee, florida, silicon, france, lake, clara, chicago, virginia, palm, los, barcelona, joseph, haven, paul, dc, zealand, ana, salt, poland, arkansas, atlanta, kong, kansas, spain, rock, chile, norway, valley, japan, arizona, miami, york, liverpool, seattle, greece, saint, berkeley, missouri, albuquerque, las, tokyo, orange, united, turkey, jefferson, washington, dallas, florence, portland, vietnam, mississippi, georgia, alabama, bangkok, austria, manchester, finland, alaska, netherlands, sweden, park, louis, lakewood, welli...Set(china, paraguay, minnesota, tallahassee, diego, town, nashville, philadelphia, angeles, germany, iowa, nevada, illinois, massachusetts, europe, kingdom, connecticut, colorado, island, district, city, delhi, dakota, boston, tennessee, florida, silicon, france, lake, clara, chicago, virginia, palm, los, barcelona, joseph, haven, paul, dc, zealand, ana, salt, poland, arkansas, atlanta, kong, kansas, spain, rock, chile, norway, valley, japan, arizona, miami, york, liverpool, seattle, greece, saint, berkeley, missouri, albuquerque, las, tokyo, orange, united, turkey, jefferson, washington, dallas, florence, portland, vietnam, mississippi, georgia, alabama, bangkok, austria, manchester, finland, alaska, netherlands, sweden, park, louis, lakewood, wellington, sydney, louisiana, cleveland, australia, des, hague, richmond, hollywood, kentucky, jose, antonio, america, springs, wichita, california, detroit, romania, texas, columbia, san, argentina, carolina, states, idaho, athens, austin, maryland, jersey, north, jordan, ohio, el, pensacola, shreveport, paris, phoenix, frankfurt, oregon, mexico, delaware, santa, hong, england, brazil, asia, maria, buffalo, korea, melbourne, peru, czech, pennsylvania, italy, canada, venice, hampton, portugal, brussels, barbara, francisco, montgomery, new, michigan, baltimore, india, glasgow, south, uruguay, johannesburg, london, boise, little, cape)\n"
      },
      "dateCreated": "Jul 27, 2016 4:06:58 AM",
      "dateStarted": "Aug 2, 2016 2:30:13 AM",
      "dateFinished": "Aug 2, 2016 2:30:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph contains the search query interface\n\nvar query \u003d z.input(\"Enter Location\", \"baltimore\").toString\n\nif(query.contains(\"new\") || query.contains(\"san\")){                                      //perform slight filtering on the input to get in retrieval form\n    query \u003d query.substring(4)\n}\n\nvar result \u003d \"\"\n{                                                        //check for exceptions if the value was not found in the index\n    try {                                                //try to get the value of the search query from the index\n        result \u003d cntxtMap(query).collect.mkString(\", \")\n    }\n    catch {\n        case ex : java.util.NoSuchElementException \u003d\u003e println(\"The location was not found in the search index.\")\n    }\n}\nprintln(\"Context of location \" + query + \" : \" + result)\n",
      "authenticationInfo": {},
      "dateUpdated": "Aug 2, 2016 3:03:20 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {
          "Enter Location": "barcelona"
        },
        "forms": {
          "Enter Location": {
            "name": "Enter Location",
            "displayName": "Enter Location",
            "type": "input",
            "defaultValue": "baltimore",
            "hidden": false
          }
        }
      },
      "apps": [],
      "jobName": "paragraph_1469593357365_1771226733",
      "id": "20160727-042237_1336488480",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "query: String \u003d barcelona\nresult: String \u003d \"\"\nContext of location barcelona : warc10, 11, tracks, enrique, iglesiasnuclear, assaultelvis, crespo, south, america, spanish, barcelona, latin, s, published, january, 01, 2015, by, ececcece, latinx\n"
      },
      "dateCreated": "Jul 27, 2016 4:22:37 AM",
      "dateStarted": "Aug 2, 2016 3:03:20 AM",
      "dateFinished": "Aug 2, 2016 3:07:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 6. Linking Entities in Common Crawl Datasets onto Wikipedia Concepts\n[Image]\n\nEvery **entity** that is mentioned in the web crawl data can be linked to one or more **contexts** on *wikipedia*, here we attempt to examine the crawl segemnts of May to extract the entities and\nmap them to their corresponding wikipedia concepts and at the same time derive a probability of how relevant or important a concept is for that particular entity since multiple entities may  have \nthe same contexts. This is done by first gathering all the entities after raw extraction and then grouping the contexts according to their entities and displaying the results in a visual  format.\nThe more the occurance of a particular context for an entity, the more are the chances of that context representing that entity accurately.\n\nThe first part in mapping the entities to their context involves the extraction of the entities themselves. This is inspired by the work of Chris Hans who has explained that the mapping of entities \nto wikipedia contexts involves extraction of all the links in the crawl that belong to \u0027wikipedia\u0027 domain. These are extracted as strings by using the warcbase library. The entities are extracted\nfrom these URL strings by the using the Stanford NLP software that recognizes entities in plain strings. Once the entities are found, they are counted in total and the links for each  entity  are\nsummed up to calculate the probability values of how strongly a context represents an entity. Finally, the generated data is formatted in the form of JSON string and is displayed in the  form  of\nan indented tree. The visualization tree displayes the entities and the links branching off from the entities with the probability values displayed alongside each link.",
      "authenticationInfo": {},
      "dateUpdated": "Aug 6, 2016 5:44:24 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469594294892_-817860295",
      "id": "20160727-043814_618039290",
      "dateCreated": "Jul 27, 2016 4:38:14 AM",
      "dateStarted": "Aug 1, 2016 5:04:24 PM",
      "dateFinished": "Aug 1, 2016 5:04:24 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//use the May segments for the extraction of data and check for the wikipedia pages\nval mayWikiLinks \u003d mayData.keepValidPages()                                                     //extract the links from the raw crawl data\n                            .keepDomains(Set(\"en.wikipedia.org\", \"www.wikipedia.org\"))\n                            .flatMap(r \u003d\u003e ExtractLinks(r.getUrl, r.getContentString))\n\nval wikiSrcLinks \u003d mayWikiLinks.map(r \u003d\u003e r._1).filter(link \u003d\u003e link.contains(\"wikipedia\"))       //get the source links from the crawl data\nval wikiDstLinks \u003d mayWikiLinks.map(r \u003d\u003e r._2).filter(link \u003d\u003e link.contains(\"wikipedia\"))       //get the destination links from the crawl data\nval wikiLinks \u003d wikiSrcLinks.union(wikiDstLinks)                                                //get the total links that contain wikipedia in the url as the domain",
      "authenticationInfo": {},
      "dateUpdated": "Aug 6, 2016 7:57:08 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469768692241_1248658660",
      "id": "20160729-050452_1642695469",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "mayData: org.apache.spark.rdd.RDD[org.warcbase.spark.archive.io.ArchiveRecord] \u003d UnionRDD[22] at union at \u003cconsole\u003e:57\nmayWikiLinks: org.apache.spark.rdd.RDD[(String, String, String)] \u003d MapPartitionsRDD[25] at flatMap at \u003cconsole\u003e:60\nwikiSrcLinks: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[27] at filter at \u003cconsole\u003e:60\nwikiDstLinks: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[29] at filter at \u003cconsole\u003e:60\nwikiLinks: org.apache.spark.rdd.RDD[String] \u003d UnionRDD[30] at union at \u003cconsole\u003e:64\n"
      },
      "dateCreated": "Jul 29, 2016 5:04:52 AM",
      "dateStarted": "Aug 6, 2016 4:16:25 AM",
      "dateFinished": "Aug 6, 2016 4:16:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph gets the entities and the links in the usable form by using the stanford NLP classifier model\nimport edu.stanford.nlp.ie.crf.CRFClassifier\nimport edu.stanford.nlp.ling.CoreAnnotations\nimport edu.stanford.nlp.ling.CoreLabel\n\nimport java.util.List\nimport scala.collection.mutable.ArraySeq\n\nval classifierPath \u003d \"/home/ubuntu/Softwares/machinelearning/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz\"     //the classifier for the NLP operation and finding entities\nval classifier \u003d CRFClassifier.getClassifierNoExceptions(classifierPath)                                                                    //get the CRF classifier\n\ndef getEntLnkMap(entLinks : (List[CoreLabel], String)) : Seq[(String, String)] \u003d {\n\n    val entLinkSeq \u003d new ArraySeq[(String, String)](entLinks._1.size())                 //the list to hold the (entity - Link) relationship\n    for(i \u003c- 0 until entLinks._1.size()){                                               //iterate over the entities for this link\n        entLinkSeq(i) \u003d (entLinks._1.get(i).toString, entLinks._2)\n    }\n    return entLinkSeq\n}\n\nval cleanLinks \u003d wikiLinks.map(urlString \u003d\u003e (urlString.replace(\"https:\", \"\") .replace(\"en.wikipedia.org\", \"\") .replace(\"www.wikipedia.org\", \"\") .replace(\".\", \" \")\n                                                      .replace(\"www.wikipedia.org/en\", \"\") .replace(\"/\", \" \") .replace(\"(\", \"\") .replace(\")\", \"\") .replace(\"\u0026\", \" \")\n                                                      .replace(\":\", \" \") .replace(\"_\", \" \") .replace(\"-\", \" \") .replace(\"\u003d\", \" \"), urlString))\nval cLinkSize \u003d cleanLinks.count.toInt\nval entitySeq \u003d new ArraySeq[(List[List[CoreLabel]], String)](cLinkSize)                //the sequence to get entities extracted from the urlStrings\nvar i \u003d 0\n\n// use iterator to get the values of the RDD and then perform the rest of the operations\nval urlIterator \u003d cleanLinks.toLocalIterator                                            //get the iterator for this RDD\n    while(urlIterator.hasNext){\n        val urlString \u003d urlIterator.next                                                //get the urlStrings\n        entitySeq(i) \u003d (classifier.classify(urlString._1), urlString._2)\n        i +\u003d 1\n    }\n\nval sentencSeq \u003d entitySeq.map(entity \u003d\u003e (entity._1.get(0), entity._2))\nval entLinkMap \u003d sentencSeq.flatMap(senten \u003d\u003e getEntLnkMap(senten))                     //get the entities and the links in (entity, link) form\n\n//get the entity-link Map as an RDD to count the entities and the get the probabilities corresponding to each link for each entity\n\nval entLinkRdd \u003d sc.parallelize(entLinkMap)                                             //get the entLinkMap as an RDD\n\nval entityCount \u003d entLinkRdd.map(ele \u003d\u003e ele._1) .map(ele \u003d\u003e (ele, 1)) .reduceByKey((x, y) \u003d\u003e x + y)             //get the count of the unique entities\nval linkCount   \u003d entLinkRdd.map(ele \u003d\u003e ((ele._1, ele._2), 1)) .reduceByKey((x, y) \u003d\u003e x + y)                    //get the count of the entity and specific links\nval entLinkKeys \u003d linkCount.map(ele \u003d\u003e (ele._1._1, (ele._1._2, ele._2)))                                        //get the entites as the keys of the RDD\nval entLnkCount \u003d entLinkKeys.join(entityCount)                                                                 //get the count of each entity and the corresponding links to be able to get probabilities\nval entLinkProb \u003d entLnkCount.map(ele \u003d\u003e (ele._1, (ele._2._1._1, (ele._2._1._2 * 1.0)/ele._2._2)))              //get the data in the probabilities form",
      "authenticationInfo": {},
      "dateUpdated": "Aug 6, 2016 4:16:33 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469768870351_1259091208",
      "id": "20160729-050750_1126139486",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import edu.stanford.nlp.ie.crf.CRFClassifier\nimport edu.stanford.nlp.ling.CoreAnnotations\nimport edu.stanford.nlp.ling.CoreLabel\nimport java.util.List\nimport scala.collection.mutable.ArraySeq\nclassifierPath: String \u003d /home/ubuntu/Softwares/machinelearning/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz\nclassifier: edu.stanford.nlp.ie.crf.CRFClassifier[edu.stanford.nlp.ling.CoreLabel] \u003d edu.stanford.nlp.ie.crf.CRFClassifier@23d25dd0\ngetEntLnkMap: (entLinks: (java.util.List[edu.stanford.nlp.ling.CoreLabel], String))Seq[(String, String)]\ncleanLinks: org.apache.spark.rdd.RDD[(String, String)] \u003d MapPartitionsRDD[31] at map at \u003cconsole\u003e:72\ncLinkSize: Int \u003d 480\nentitySeq: scala.collection.mutable.ArraySeq[(java.util.List[java.util.List[edu.stanford.nlp.ling.CoreLabel]], String)] \u003d ArraySeq(null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, ...i: Int \u003d 0\nurlIterator: Iterator[(String, String)] \u003d non-empty iterator\nsentencSeq: scala.collection.mutable.ArraySeq[(java.util.List[edu.stanford.nlp.ling.CoreLabel], String)] \u003d ArraySeq(([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_Model_Canvas), ([wiki, Business, Model, Canvas],https://en.wikipedia.org/wiki/Business_...entLinkMap: scala.collection.mutable.ArraySeq[(String, String)] \u003d ArraySeq((wiki,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Business,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Model,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Canvas,https://en.wikipedia.org/wiki/Business_Model_Canvas), (wiki,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Business,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Model,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Canvas,https://en.wikipedia.org/wiki/Business_Model_Canvas), (wiki,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Business,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Model,https://en.wikipedia.org/wiki/Business_Model_Canvas), (Canvas,https://en.wikipedia.org/wiki/Busin...entLinkRdd: org.apache.spark.rdd.RDD[(String, String)] \u003d ParallelCollectionRDD[32] at parallelize at \u003cconsole\u003e:84\nentityCount: org.apache.spark.rdd.RDD[(String, Int)] \u003d ShuffledRDD[35] at reduceByKey at \u003cconsole\u003e:86\nlinkCount: org.apache.spark.rdd.RDD[((String, String), Int)] \u003d ShuffledRDD[37] at reduceByKey at \u003cconsole\u003e:86\nentLinkKeys: org.apache.spark.rdd.RDD[(String, (String, Int))] \u003d MapPartitionsRDD[38] at map at \u003cconsole\u003e:88\nentLnkCount: org.apache.spark.rdd.RDD[(String, ((String, Int), Int))] \u003d MapPartitionsRDD[41] at join at \u003cconsole\u003e:92\nentLinkProb: org.apache.spark.rdd.RDD[(String, (String, Double))] \u003d MapPartitionsRDD[42] at map at \u003cconsole\u003e:94\n"
      },
      "dateCreated": "Jul 29, 2016 5:07:50 AM",
      "dateStarted": "Aug 6, 2016 4:16:33 AM",
      "dateFinished": "Aug 6, 2016 4:41:41 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph gets the data in the json representation format\n\ndef getTreeJson(entLinks : Array[(String, (String, Double))]) : String \u003d {              //the json formatting function for converting to json type.\n    \n    var entities \u003d Array[String]()                              //the string to hold the entities and their links individually\n    var links  \u003d Array[String]()                                //the string array to hold the links for an entity \n    var currEnt \u003d entLinks(0)._1                                //string to hold the current entity ; get the first entity\n    \n    for(i \u003c- 0 until entLinks.length){\n        \n        if(!(currEnt.equals(entLinks(i)._1))){\n            entities \u003d entities.:+(\"{ \\\"name\\\" : \\\"\" + currEnt + \"\\\", \\\"children\\\" : [ \" + links.mkString(\",\") + \" ] }\")\n            currEnt \u003d entLinks(i)._1                            //change the entity to the current entity\n            links \u003d Array[String]()                             //renew the links array for the new entity\n        }\n                                                                //create the link type for each link of each entity\n        links \u003d links.:+(\"{ \\\"name\\\" : \\\"\" + entLinks(i)._2._1 + \"( \" + entLinks(i)._2._2 + \" ) \\\", \\\"size\\\" : \" + (entLinks(i)._2._2 * 1000) + \"}\")\n    }\n    entities \u003d entities.:+(\"{ \\\"name\\\" : \\\"\" + currEnt + \"\\\", \\\"children\\\" : [\" + links.mkString(\",\") + \"] }\")         //get the values for the last entity\n    \n    return \"{ \\\"name\\\" : \\\"entities\\\", \\\"children\\\" : [ \" + entities.mkString(\",\") + \" ] }\"                               //return the json representation of the entities\n}\n\nval entLinkArr \u003d entLinkProb.collect()                                                  //collect the entity-link records into an array\nval entLnkJson \u003d getTreeJson(entLinkArr)                                                //get the values as string in the json format",
      "authenticationInfo": {},
      "dateUpdated": "Aug 5, 2016 5:32:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469770709194_1095777597",
      "id": "20160729-053829_2047575871",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "getTreeJson: (entLinks: Array[(String, (String, Double))])String\nentLinkArr: Array[(String, (String, Double))] \u003d Array((Q5565037,(https://www.wikidata.org/wiki/Q5565037#sitelinks-wikipedia,1.0)), (Karl,(https://en.wikipedia.org/wiki/Karl_Lagerfeld,0.9848484848484849)), (Karl,(https://en.wikipedia.org/w/index.php?title\u003dKarl_Lagerfeld\u0026oldid\u003d722498966,0.015151515151515152)), (Business,(https://en.wikipedia.org/w/index.php?title\u003dBusiness_Model_Canvas\u0026oldid\u003d721351875,0.043478260869565216)), (Business,(https://en.wikipedia.org/wiki/Business_Model_Canvas,0.9565217391304348)), (Q76716,(https://www.wikidata.org/wiki/Q76716#sitelinks-wikipedia,1.0)), (campaign,(https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source\u003ddonate\u0026utm_medium\u003dsidebar\u0026utm_campaign\u003dC13_en.wikipedia.org\u0026uselang\u003den,1.0)), (Bernard,(https://en.wikipedia.org/wiki/George_Ber...entLnkJson: String \u003d { \"name\" : \"entities\", \"children\" : [ { \"name\" : \"Q5565037\", \"children\" : [ { \"name\" : \"https://www.wikidata.org/wiki/Q5565037#sitelinks-wikipedia( 1.0 ) \", \"size\" : 1000.0} ] },{ \"name\" : \"Karl\", \"children\" : [ { \"name\" : \"https://en.wikipedia.org/wiki/Karl_Lagerfeld( 0.9848484848484849 ) \", \"size\" : 984.8484848484849},{ \"name\" : \"https://en.wikipedia.org/w/index.php?title\u003dKarl_Lagerfeld\u0026oldid\u003d722498966( 0.015151515151515152 ) \", \"size\" : 15.151515151515152} ] },{ \"name\" : \"Business\", \"children\" : [ { \"name\" : \"https://en.wikipedia.org/w/index.php?title\u003dBusiness_Model_Canvas\u0026oldid\u003d721351875( 0.043478260869565216 ) \", \"size\" : 43.47826086956522},{ \"name\" : \"https://en.wikipedia.org/wiki/Business_Model_Canvas( 0.9565217391304348 ) \", \"size\" : 956.5217391304349} ] },{..."
      },
      "dateCreated": "Jul 29, 2016 5:38:29 AM",
      "dateStarted": "Aug 5, 2016 5:32:57 PM",
      "dateFinished": "Aug 5, 2016 5:32:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Entities and their Wikipedia Links - Indented Tree",
      "text": "//this paragraph prints the indented tree for the entity - link lists\nprintln(\"\"\"%html\n\u003c!DOCTYPE html\u003e\n\u003cstyle\u003e\n    #treeChart{\n      height: 3000px;\n    }\n    .node rect {\n      cursor: pointer;\n      fill: #fff;\n      fill-opacity: .5;\n      stroke: #3f19ac;\n      stroke-width: 1.5px;\n    }\n    .node text {\n      font: 10px sans-serif;\n      pointer-events: none;\n    }\n    path.link {\n      fill: none;\n      stroke: #3f19ac;\n      stroke-width: 1.5px;\n    }\n\u003c/style\u003e\n\n\u003cdiv id\u003d\"treeChart\"\u003e\u003c/div\u003e\n\n\u003cscript\u003e\n    var margin \u003d {top: 30, right: 20, bottom: 30, left: 20},\n        width \u003d 960 - margin.left - margin.right,\n        barHeight \u003d 20,\n        barWidth \u003d width * 1.1;\n    \n    var i \u003d 0,\n        duration \u003d 400,\n        root;\n    \n    var tree \u003d d3.layout.tree()\n        .nodeSize([0, 20]);\n    \n    var diagonal \u003d d3.svg.diagonal()\n        .projection(function(d) { return [d.y, d.x]; });\n    \n    var svg \u003d d3.select(\"#treeChart\").append(\"svg\")\n        .attr(\"width\", width + margin.left + margin.right)\n      .append(\"g\")\n        .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n    \n    d3.json(\"treedata.json\", function(error, flare) {\n      if (error) throw error;\n    \n      flare.x0 \u003d 0;\n      flare.y0 \u003d 0;\n      update(root \u003d flare);\n    });\n    \n    function update(source) {\n    \n      // Compute the flattened node list. TODO use d3.layout.hierarchy.\n      var nodes \u003d tree.nodes(root);\n    \n      var height \u003d Math.max(500, nodes.length * barHeight + margin.top + margin.bottom);\n    \n      d3.select(\"svg\").transition()\n          .duration(duration)\n          .attr(\"height\", height);\n    \n      d3.select(self.frameElement).transition()\n          .duration(duration)\n          .style(\"height\", height + \"px\");\n    \n      // Compute the \"layout\".\n      nodes.forEach(function(n, i) {\n        n.x \u003d i * barHeight;\n      });\n    \n      // Update the nodesâ€¦\n      var node \u003d svg.selectAll(\"g.node\")\n          .data(nodes, function(d) { return d.id || (d.id \u003d ++i); });\n    \n      var nodeEnter \u003d node.enter().append(\"g\")\n          .attr(\"class\", \"node\")\n          .attr(\"transform\", function(d) { return \"translate(\" + source.y0 + \",\" + source.x0 + \")\"; })\n          .style(\"opacity\", 1e-6);\n    \n      // Enter any new nodes at the parent\u0027s previous position.\n      nodeEnter.append(\"rect\")\n          .attr(\"y\", -barHeight / 2)\n          .attr(\"height\", barHeight)\n          .attr(\"width\", barWidth)\n          .style(\"fill\", color)\n          .on(\"click\", click);\n    \n      nodeEnter.append(\"text\")\n          .attr(\"dy\", 3.5)\n          .attr(\"dx\", 5.5)\n          .text(function(d) { return d.name; });\n    \n      // Transition nodes to their new position.\n      nodeEnter.transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + d.y + \",\" + d.x + \")\"; })\n          .style(\"opacity\", 1);\n    \n      node.transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + d.y + \",\" + d.x + \")\"; })\n          .style(\"opacity\", 1)\n        .select(\"rect\")\n          .style(\"fill\", color);\n    \n      // Transition exiting nodes to the parent\u0027s new position.\n      node.exit().transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + source.y + \",\" + source.x + \")\"; })\n          .style(\"opacity\", 1e-6)\n          .remove();\n    \n      // Update the linksâ€¦\n      var link \u003d svg.selectAll(\"path.link\")\n          .data(tree.links(nodes), function(d) { return d.target.id; });\n    \n      // Enter any new links at the parent\u0027s previous position.\n      link.enter().insert(\"path\", \"g\")\n          .attr(\"class\", \"link\")\n          .attr(\"d\", function(d) {\n            var o \u003d {x: source.x0, y: source.y0};\n            return diagonal({source: o, target: o});\n          })\n        .transition()\n          .duration(duration)\n          .attr(\"d\", diagonal);\n    \n      // Transition links to their new position.\n      link.transition()\n          .duration(duration)\n          .attr(\"d\", diagonal);\n    \n      // Transition exiting nodes to the parent\u0027s new position.\n      link.exit().transition()\n          .duration(duration)\n          .attr(\"d\", function(d) {\n            var o \u003d {x: source.x, y: source.y};\n            return diagonal({source: o, target: o});\n          })\n          .remove();\n    \n      // Stash the old positions for transition.\n      nodes.forEach(function(d) {\n        d.x0 \u003d d.x;\n        d.y0 \u003d d.y;\n      });\n    }\n    \n    // Toggle children on click.\n    function click(d) {\n      if (d.children) {\n        d._children \u003d d.children;\n        d.children \u003d null;\n      } else {\n        d.children \u003d d._children;\n        d._children \u003d null;\n      }\n      update(d);\n    }\n    \n    function color(d) {\n      return d._children ? \"#343746\" : d.children ? \"#3f39bc\" : \"#bcbbf9\";\n    }\n\n\u003c/script\u003e\n\"\"\")",
      "authenticationInfo": {},
      "dateUpdated": "Aug 6, 2016 5:23:43 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1470326311150_1564484674",
      "id": "20160804-155831_2066546481",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003c!DOCTYPE html\u003e\n\u003cstyle\u003e\n    #treeChart{\n      height: 3000px;\n    }\n    .node rect {\n      cursor: pointer;\n      fill: #fff;\n      fill-opacity: .5;\n      stroke: #3f19ac;\n      stroke-width: 1.5px;\n    }\n    .node text {\n      font: 10px sans-serif;\n      pointer-events: none;\n    }\n    path.link {\n      fill: none;\n      stroke: #3f19ac;\n      stroke-width: 1.5px;\n    }\n\u003c/style\u003e\n\n\u003cdiv id\u003d\"treeChart\"\u003e\u003c/div\u003e\n\n\u003cscript\u003e\n    var margin \u003d {top: 30, right: 20, bottom: 30, left: 20},\n        width \u003d 960 - margin.left - margin.right,\n        barHeight \u003d 20,\n        barWidth \u003d width * 1.1;\n    \n    var i \u003d 0,\n        duration \u003d 400,\n        root;\n    \n    var tree \u003d d3.layout.tree()\n        .nodeSize([0, 20]);\n    \n    var diagonal \u003d d3.svg.diagonal()\n        .projection(function(d) { return [d.y, d.x]; });\n    \n    var svg \u003d d3.select(\"#treeChart\").append(\"svg\")\n        .attr(\"width\", width + margin.left + margin.right)\n      .append(\"g\")\n        .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n    \n    d3.json(\"treedata.json\", function(error, flare) {\n      if (error) throw error;\n    \n      flare.x0 \u003d 0;\n      flare.y0 \u003d 0;\n      update(root \u003d flare);\n    });\n    \n    function update(source) {\n    \n      // Compute the flattened node list. TODO use d3.layout.hierarchy.\n      var nodes \u003d tree.nodes(root);\n    \n      var height \u003d Math.max(500, nodes.length * barHeight + margin.top + margin.bottom);\n    \n      d3.select(\"svg\").transition()\n          .duration(duration)\n          .attr(\"height\", height);\n    \n      d3.select(self.frameElement).transition()\n          .duration(duration)\n          .style(\"height\", height + \"px\");\n    \n      // Compute the \"layout\".\n      nodes.forEach(function(n, i) {\n        n.x \u003d i * barHeight;\n      });\n    \n      // Update the nodesâ€¦\n      var node \u003d svg.selectAll(\"g.node\")\n          .data(nodes, function(d) { return d.id || (d.id \u003d ++i); });\n    \n      var nodeEnter \u003d node.enter().append(\"g\")\n          .attr(\"class\", \"node\")\n          .attr(\"transform\", function(d) { return \"translate(\" + source.y0 + \",\" + source.x0 + \")\"; })\n          .style(\"opacity\", 1e-6);\n    \n      // Enter any new nodes at the parent\u0027s previous position.\n      nodeEnter.append(\"rect\")\n          .attr(\"y\", -barHeight / 2)\n          .attr(\"height\", barHeight)\n          .attr(\"width\", barWidth)\n          .style(\"fill\", color)\n          .on(\"click\", click);\n    \n      nodeEnter.append(\"text\")\n          .attr(\"dy\", 3.5)\n          .attr(\"dx\", 5.5)\n          .text(function(d) { return d.name; });\n    \n      // Transition nodes to their new position.\n      nodeEnter.transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + d.y + \",\" + d.x + \")\"; })\n          .style(\"opacity\", 1);\n    \n      node.transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + d.y + \",\" + d.x + \")\"; })\n          .style(\"opacity\", 1)\n        .select(\"rect\")\n          .style(\"fill\", color);\n    \n      // Transition exiting nodes to the parent\u0027s new position.\n      node.exit().transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) { return \"translate(\" + source.y + \",\" + source.x + \")\"; })\n          .style(\"opacity\", 1e-6)\n          .remove();\n    \n      // Update the linksâ€¦\n      var link \u003d svg.selectAll(\"path.link\")\n          .data(tree.links(nodes), function(d) { return d.target.id; });\n    \n      // Enter any new links at the parent\u0027s previous position.\n      link.enter().insert(\"path\", \"g\")\n          .attr(\"class\", \"link\")\n          .attr(\"d\", function(d) {\n            var o \u003d {x: source.x0, y: source.y0};\n            return diagonal({source: o, target: o});\n          })\n        .transition()\n          .duration(duration)\n          .attr(\"d\", diagonal);\n    \n      // Transition links to their new position.\n      link.transition()\n          .duration(duration)\n          .attr(\"d\", diagonal);\n    \n      // Transition exiting nodes to the parent\u0027s new position.\n      link.exit().transition()\n          .duration(duration)\n          .attr(\"d\", function(d) {\n            var o \u003d {x: source.x, y: source.y};\n            return diagonal({source: o, target: o});\n          })\n          .remove();\n    \n      // Stash the old positions for transition.\n      nodes.forEach(function(d) {\n        d.x0 \u003d d.x;\n        d.y0 \u003d d.y;\n      });\n    }\n    \n    // Toggle children on click.\n    function click(d) {\n      if (d.children) {\n        d._children \u003d d.children;\n        d.children \u003d null;\n      } else {\n        d.children \u003d d._children;\n        d._children \u003d null;\n      }\n      update(d);\n    }\n    \n    function color(d) {\n      return d._children ? \"#343746\" : d.children ? \"#3f39bc\" : \"#bcbbf9\";\n    }\n\n\u003c/script\u003e\n\n"
      },
      "dateCreated": "Aug 4, 2016 3:58:31 PM",
      "dateStarted": "Aug 6, 2016 5:23:18 AM",
      "dateFinished": "Aug 6, 2016 5:23:19 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 7. Text Based Search Engine\n[Image]\n\nThis section attempts to create a search engine using Apache Lucene. Since not all of the data can be accomodated on the EC2 instance that is used to run this search engine, domains have been\nfiltered from the crawl data. As required for desiging a search engine the first paragraph below creates an index of the web page documents and stores it on the hard disk directory of the ins\n-tance. The next paragraph retrives the index to create the \u0027Search Indexer\u0027 and display the searched results as an interface in the subsequent paragraph.",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 4:30:06 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1469770965145_981105456",
      "id": "20160729-054245_803271288",
      "dateCreated": "Jul 29, 2016 5:42:45 AM",
      "dateStarted": "Aug 5, 2016 8:07:40 AM",
      "dateFinished": "Aug 5, 2016 8:07:40 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph generates the data and creates the index of the search engine from the data\nimport org.apache.lucene.store.{Directory, FSDirectory}\nimport org.apache.lucene.analysis.standard.StandardAnalyzer\nimport org.apache.lucene.document.{Document, Field, TextField}\nimport org.apache.lucene.index.{IndexWriter, IndexWriterConfig}\n\nimport java.io.File\n\nval pagesDoc \u003d mayData.keepValidPages()\n                      .keepDomains(Set(\"www.fangraphs.com\", \"www.osnwes.com\", \"www.google.com\", \"en.wikipedia.org\"))\n                      .map(page \u003d\u003e (page.getUrl, page.getContentString))                         //get the valid pages out of the May data\n\nval analyser \u003d new StandardAnalyzer();\n\nval indexDir \u003d new File(\"/home/ubuntu/ZeppelinData/LuceneIndex\")                     //the directory to contain the generated index\nval directory \u003d FSDirectory.open(indexDir.toPath())\n\nval config \u003d new IndexWriterConfig(analyser)                    //the index writer configuration for the index\nval idxWriter \u003d new IndexWriter(directory, config)              //the index writer for the index derived from the input files\n\nval pageIter \u003d pagesDoc.toLocalIterator                         //get the iterator on the web pages to get each page\n\n    while(pageIter.hasNext){                                    //iterate over all the pages to get the document corresponding to each page\n        val page \u003d pageIter.next\n        val doc \u003d new Document()\n        doc.add(new Field(\"url\", page._1, TextField.TYPE_STORED))\n        doc.add(new Field(\"content\", page._2, TextField.TYPE_STORED))\n        idxWriter.addDocument(doc)\n    }\n\nidxWriter.close()",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 3:41:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1470373994450_-1769236045",
      "id": "20160805-051314_384172667",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Aug 5, 2016 5:13:14 AM",
      "dateStarted": "Aug 6, 2016 5:07:34 PM",
      "dateFinished": "Aug 6, 2016 5:15:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//this paragraph instantiates the required classes for reading from the already created index \nimport org.apache.lucene.document.Document\nimport org.apache.lucene.store.FSDirectory\nimport org.apache.lucene.search.{IndexSearcher, Query}\nimport org.apache.lucene.queryparser.classic.QueryParser\nimport org.apache.lucene.analysis.standard.StandardAnalyzer\nimport org.apache.lucene.index.{IndexReader, DirectoryReader}\n\nimport java.io.File\n\nval analyser \u003d new StandardAnalyzer()\n\nval indexDir \u003d new File(\"/home/ubuntu/ZeppelinData/LuceneIndex\")\nval idxReader \u003d DirectoryReader.open(FSDirectory.open(indexDir.toPath()))                                   //read the index directory\nval idxSearcher \u003d new IndexSearcher(idxReader)                                                              //the index Searcher to search the index\nval qParser \u003d new QueryParser(\"content\", analyser)              //the query parser to parse the incoming queries and get them in the required form\n",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 3:39:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1470403869827_-1946683619",
      "id": "20160805-133109_2044005162",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.lucene.document.Document\nimport org.apache.lucene.store.FSDirectory\nimport org.apache.lucene.search.{IndexSearcher, Query}\nimport org.apache.lucene.queryparser.classic.QueryParser\nimport org.apache.lucene.analysis.standard.StandardAnalyzer\nimport org.apache.lucene.index.{IndexReader, DirectoryReader}\nimport java.io.File\nanalyser: org.apache.lucene.analysis.standard.StandardAnalyzer \u003d org.apache.lucene.analysis.standard.StandardAnalyzer@19bc58ed\nindexDir: java.io.File \u003d /home/ubuntu/ZeppelinData/LuceneIndex\nidxReader: org.apache.lucene.index.DirectoryReader \u003d StandardDirectoryReader(segments_1:6 _0(6.1.0):c240 _1(6.1.0):c78)\nidxSearcher: org.apache.lucene.search.IndexSearcher \u003d IndexSearcher(StandardDirectoryReader(segments_1:6 _0(6.1.0):c240 _1(6.1.0):c78); executor\u003dnull)\nqParser: org.apache.lucene.queryparser.classic.QueryParser \u003d org.apache.lucene.queryparser.classic.QueryParser@1846c014\n"
      },
      "dateCreated": "Aug 5, 2016 1:31:09 PM",
      "dateStarted": "Aug 7, 2016 3:39:50 AM",
      "dateFinished": "Aug 7, 2016 3:39:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Zeppelin Search Engine",
      "text": "//this paragraph presents the interface for searching by taking the query and returning the results\n\nvar inpQry \u003d z.input(\"Enter Search Query\", \"fangraphs\").toString                                                //get the search from the user\nval query \u003d qParser.parse(inpQry)\n\nval pageHits \u003d idxSearcher.search(query, 10).scoreDocs          //get the top ten results for the query from the search index\n\nprintln(s\"%html \u003ch3\u003eResults for the search query \u0027$inpQry\u0027 \u003c/h3\u003e \u003cbr/\u003e \")\n    for(i \u003c- 0 until pageHits.length){                          //iterate through the results to display the pages that can be used for searching\n        val doc \u003d idxSearcher.doc(pageHits(i).doc)              //get the documents corresponding to the indexes\n        val urlString \u003d doc.get(\"url\")\n        println(s\"\"\"%html \u003ca href\u003d\"$urlString\"\u003e$urlString\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\"\"\")                                        //get the url corresponding to each of the pages/documents retrieved\n    }",
      "authenticationInfo": {},
      "dateUpdated": "Aug 7, 2016 4:24:23 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {
          "Enter Search Query": "mac"
        },
        "forms": {
          "Enter Search Query": {
            "name": "Enter Search Query",
            "displayName": "Enter Search Query",
            "type": "input",
            "defaultValue": "fangraphs",
            "hidden": false
          }
        }
      },
      "apps": [],
      "jobName": "paragraph_1470470650080_-1972948421",
      "id": "20160806-080410_1348915723",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eResults for the search query \u0027mac\u0027 \u003c/h3\u003e \u003cbr/\u003e \n\u003ca href\u003d\"http://www.google.com/patents/US5905859?dq\u003d5,666,293\"\u003ehttp://www.google.com/patents/US5905859?dq\u003d5,666,293\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\n\u003ca href\u003d\"http://www.google.com/patents/US6157955?dq\u003d7,599,983\"\u003ehttp://www.google.com/patents/US6157955?dq\u003d7,599,983\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\n\u003ca href\u003d\"https://en.wikipedia.org/wiki/Ireland\"\u003ehttps://en.wikipedia.org/wiki/Ireland\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\n\u003ca href\u003d\"http://www.google.com/patents/US7778176?dq\u003dabelow\"\u003ehttp://www.google.com/patents/US7778176?dq\u003dabelow\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\n\u003ca href\u003d\"http://www.fangraphs.com/leaders.aspx?pos\u003dall\u0026stats\u003dpit\u0026lg\u003dall\u0026qual\u003dy\u0026type\u003d0\u0026season\u003d2003\u0026month\u003d0\u0026season1\u003d1993\u0026ind\u003d0\u0026team\u003d0\u0026rost\u003d0\u0026age\u003d0\u0026filter\u003d\u0026players\u003d0\u0026sort\u003d5,d\"\u003ehttp://www.fangraphs.com/leaders.aspx?pos\u003dall\u0026stats\u003dpit\u0026lg\u003dall\u0026qual\u003dy\u0026type\u003d0\u0026season\u003d2003\u0026month\u003d0\u0026season1\u003d1993\u0026ind\u003d0\u0026team\u003d0\u0026rost\u003d0\u0026age\u003d0\u0026filter\u003d\u0026players\u003d0\u0026sort\u003d5,d\u003c/a\u003e \u003cbr/\u003e\u003cbr/\u003e\n"
      },
      "dateCreated": "Aug 6, 2016 8:04:10 AM",
      "dateStarted": "Aug 7, 2016 4:23:20 AM",
      "dateFinished": "Aug 7, 2016 4:23:20 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n[Image - CommonCrawl]\nThis notebook presented the possible forms of analysis that could be done with the Common Crawl DataSet ending with a fully funtional search engine for domains that were filtered for\nmonth of May. With the amount of data that is generated, collecting and storing it is a herculean task and commercial companies have sole access to their data. The Common Crawl  data\ncorpus is definitely an asset to those who want to use web crawl data in their own applications, moreover, it\u0027s organized and is available for free. In the times to come, it is hoped\nthat the usage and availability of such important and precious data will be more.",
      "dateUpdated": "Aug 7, 2016 4:05:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1470541217494_-1271908118",
      "id": "20160807-034017_1718754932",
      "dateCreated": "Aug 7, 2016 3:40:17 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "WorldWideWeb",
  "id": "2BSD5NUW1",
  "angularObjects": {
    "2BPV8SYCR:shared_process": [],
    "2BQY4H7KP:shared_process": [],
    "2BQ94M8XE:shared_process": [],
    "2BRPGMCVY:shared_process": [],
    "2BTGH6J1E:shared_process": [],
    "2BQVNEWQH:shared_process": [],
    "2BS6FEEZP:shared_process": [],
    "2BRNVU7RK:shared_process": [],
    "2BRYHD875:shared_process": [],
    "2BR8UCVZJ:shared_process": [],
    "2BQVSK9XW:shared_process": [],
    "2BRE76T2Z:shared_process": [],
    "2BRAMBNCV:shared_process": [],
    "2BS58EH64:shared_process": [],
    "2BS7KGCEQ:shared_process": [],
    "2BSW2PR8X:shared_process": [],
    "2BRQ172X7:shared_process": [],
    "2BSFSRXUA:shared_process": []
  },
  "config": {},
  "info": {}
}